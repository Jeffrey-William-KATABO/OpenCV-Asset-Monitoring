{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgx-88wo1VVA"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "import paho.mqtt.client as mqtt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWtFwG3D1VVD"
   },
   "outputs": [],
   "source": [
    "# Loading the trained model from the .h5 file\n",
    "model = tf.keras.models.load_model('modele.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUNbhAtCP1f7"
   },
   "outputs": [],
   "source": [
    "# Initializing ORB object\n",
    "orb = cv2.ORB_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTT7i7eO26DA"
   },
   "outputs": [],
   "source": [
    "# Setting up video capture from the webcam\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAK9Ay7bY5E4"
   },
   "outputs": [],
   "source": [
    "# Defining data hyperparameters for image preprocessing\n",
    "data_hyperparams = {\n",
    "    'img_size': (64,64),\n",
    "    'batch_size': 8,\n",
    "    'preprocessing_function': tf.keras.applications.densenet.preprocess_input\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiVcV-liZJUx"
   },
   "outputs": [],
   "source": [
    "# Defining the image batch preprocessing function as required by the model:\n",
    "def preprocess(batch):\n",
    "  prep_batch = []\n",
    "  for img in batch:\n",
    "    preprocessed_img = tf.image.resize(img, data_hyperparams['img_size']) # Resizing the image to 64x64\n",
    "    preprocessed_img = data_hyperparams['preprocessing_function'](preprocessed_img) # Preprocessing the image with keras\n",
    "    preprocessed_img = preprocessed_img / 255.0 # Normalizing the pixel values\n",
    "    prep_batch.append(preprocessed_img) # Append the preprocessed image to the batch of processed images\n",
    "  return prep_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to determine the class of activity based on the model prediction on an individual image\n",
    "def ImageClassDecoder(prediction):\n",
    "    \"\"\"\n",
    "    Function to get the type of activity detected from the model prediction.\n",
    "    Takes an array of predictions and returns the corresponding class label and textual description.\n",
    "    \"\"\"\n",
    "    if prediction.argmax() == 0:\n",
    "        return (0, 'Abuse')\n",
    "    elif prediction.argmax() == 1:\n",
    "        return (1, 'Arrest')\n",
    "    elif prediction.argmax() == 2:\n",
    "        return (2, 'Arson')\n",
    "    elif prediction.argmax() == 3:\n",
    "        return (3, 'Assault')\n",
    "    elif prediction.argmax() == 4:\n",
    "        return (4, 'Burglary')\n",
    "    elif prediction.argmax() == 5:\n",
    "        return (5, 'Explosion')\n",
    "    elif prediction.argmax() == 6:\n",
    "        return (6, 'Fighting')\n",
    "    elif prediction.argmax() == 7:\n",
    "        return (7, 'Normal')\n",
    "    elif prediction.argmax() == 8:\n",
    "        return (8, 'Road Accident')\n",
    "    elif prediction.argmax() == 9:\n",
    "        return (9, 'Robbery')\n",
    "    elif prediction.argmax() == 10:\n",
    "        return (10, 'Shooting')\n",
    "    elif prediction.argmax() == 11:\n",
    "        return (11, 'Shoplifting')\n",
    "    elif prediction.argmax() == 12:\n",
    "        return (12, 'Stealing')\n",
    "    elif prediction.argmax() == 13:\n",
    "        return (13, 'Vandalism')\n",
    "    \n",
    "# Same thing, but for a batch of images\n",
    "    \n",
    "def BatchClassDecoder(batch):\n",
    "    \"\"\" \n",
    "    Function to determine the the class of a batch of images.\n",
    "    The batch class is the most recurrent class among the predictions on individual images.\n",
    "    \"\"\"\n",
    "    classes = []\n",
    "    counts = [0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for img in batch:\n",
    "        classes.append(ImageClassDecoder(img)[0])\n",
    "    for cls in classes:\n",
    "        counts[cls] += 1\n",
    "    c = np.array(counts).argmax()\n",
    "    if c == 0:\n",
    "        return (0, 'Abuse') # 6/10\n",
    "    elif c == 1:\n",
    "        return (1, 'Arrest') # 5/10\n",
    "    elif c == 2:\n",
    "        return (2, 'Arson') # 10/10\n",
    "    elif c == 3:\n",
    "        return (3, 'Assault') # 8/10\n",
    "    elif c == 4:\n",
    "        return (4, 'Burglary') # 10/10\n",
    "    elif c == 5:\n",
    "        return (5, 'Explosion') # 9/10\n",
    "    elif c == 6:\n",
    "        return (6, 'Fighting') # 5/10\n",
    "    elif c == 7:\n",
    "        return (7, 'Normal') # 0/10\n",
    "    elif c == 8:\n",
    "        return (8, 'Road Accident') # 8/10\n",
    "    elif c == 9:\n",
    "        return (9, 'Robbery') # 8/10\n",
    "    elif c == 10:\n",
    "        return (10, 'Shooting') # 10/10\n",
    "    elif c == 11:\n",
    "        return (11, 'Shoplifting') #7/10\n",
    "    elif c == 12:\n",
    "        return (12, 'Stealing') # 7/10\n",
    "    elif c == 13:\n",
    "        return (13, 'Vandalism') # 7/10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchProb(batch):\n",
    "    \"\"\"\n",
    "    Function to determine the prediction probability for a batch of images.\n",
    "    Takes a batch of images and returns its mean probability vector\n",
    "    \"\"\"\n",
    "    S=[]\n",
    "    for i in range(len(batch[0])):\n",
    "        s=0\n",
    "        for elt in batch:\n",
    "            s+=elt[i]\n",
    "        S.append(s/len(batch))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wotZLU--47jn"
   },
   "outputs": [],
   "source": [
    "# Creating  a Multiple Instance Learning (MIL) tracker for the Region Of Interest (ROI)\n",
    "tracker = cv2.TrackerMIL_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exN7HeZpSFaN"
   },
   "source": [
    "### A. Module I: Human Presence Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the presence of people in the images using the media pipe library \n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (0, 255, 0)  # green\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image,\n",
    "    detection_result\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualized.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    category_name = category.category_name\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = category_name + ' (' + str(probability) + ')'\n",
    "    text_location = (MARGIN + bbox.origin_x,\n",
    "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "DetectionResult = mp.tasks.components.containers.DetectionResult\n",
    "ObjectDetector = mp.tasks.vision.ObjectDetector\n",
    "ObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='efficientdet.tflite')\n",
    "\n",
    "DRS = DetectionResult(detections=[]) # Initialization of the global detection results object\n",
    "\n",
    "# Callback function invoked each time an object is detected in the video stream\n",
    "def get_result(result: DetectionResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    #print('detection result: {}'.format(result))\n",
    "    global DRS # Global detection result object, accessible from outside the function\n",
    "    DRS=result\n",
    "\n",
    "options = ObjectDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path='efficientdet.tflite'),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    max_results=1, # maximum number of objects to be detected\n",
    "    result_callback=get_result,\n",
    "    category_allowlist = 'person' # We are only interested in detecting the presence of humans\n",
    "                                  # ?? Every other category will be ignored ??\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1 # Initial timestamp\n",
    "        \n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        print(i)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.array(frame))\n",
    "        with ObjectDetector.create_from_options(options) as detector:\n",
    "            detector.detect_async(mp_image, i)\n",
    "            image_copy = np.copy(mp_image.numpy_view())\n",
    "            annotated_image = visualize(image_copy, DRS)\n",
    "            rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imshow('Video', annotated_image)\n",
    "            i+=1   \n",
    "\n",
    "        \"\"\"\n",
    "        NOTE: Be very careful with the detector.detect_async() function. Its second argument (frame timestamp)\n",
    "        MUST be GREATER than what the function has processed previously. In other words, If you use incremented \n",
    "        counters as timestamps when you first run the function, the first counter of the next execution MUST be GREATER\n",
    "        than the last counter of the function. The same applies if you use timestamps from the time module.\n",
    "        \"\"\"\n",
    "        # Defining a binary state variable that indicates whether there is a person near the asset\n",
    "        if DRS.detections[0].categories[0].category_name == 'person':\n",
    "            person = True\n",
    "    # Check for the 'q' key to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Module II: Offensive Activity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3fz68QEUQF_"
   },
   "outputs": [],
   "source": [
    "# Initialize a batch to store the frames\n",
    "frame_batch = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if ret:\n",
    "        frame_batch.append(frame)\n",
    "\n",
    "        # Checking if the frame batch size has reached the desired batch size\n",
    "        if len(frame_batch) == data_hyperparams['batch_size']:\n",
    "            \n",
    "            # Convert the frame batch to a numpy array\n",
    "            frame_batch = np.array(frame_batch)\n",
    "\n",
    "            # Perform preprocessing on the frame batch\n",
    "            frame_batch = np.array(preprocess(frame_batch))\n",
    "\n",
    "            # Pass the frame batch to the computer vision model for offensive action detection\n",
    "            predictions = model.predict(frame_batch)\n",
    "            Class = BatchClassDecoder(predictions)[1]\n",
    "            print(Class)\n",
    "            cv2.putText(frame, str(Class), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (255,0,0),2)\n",
    "            # Clear the frame batch for the next batch of frames\n",
    "            frame_batch = []\n",
    "            \n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    # Check for the 'q' key to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Module III: Alteration Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With Tracker \n",
    "\n",
    "# Reading the initial frame and select the region of interest (ROI)\n",
    "ret, frame = video_capture.read()\n",
    "bbox = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "# Extracting features from the initial state (ROI)\n",
    "roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "kp1, des1 = orb.detectAndCompute(roi, None)\n",
    "#kp1, des1 = orb.detectAndCompute(frame, None)\n",
    "# Create a kNN matcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "while True:\n",
    "    # Reading the next frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Updating the object tracker\n",
    "    ret, bbox = tracker.update(frame)\n",
    "    if ret:\n",
    "        # Extracting features from the current state (ROI)\n",
    "        roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "        kp2, des2 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "        # Performing feature matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Perform kNN matching\n",
    "        #matches = bf.knnMatch(des1, des2, k=4)\n",
    "\n",
    "        # Apply ratio test to filter good matches\n",
    "        #good_matches = []\n",
    "        #for m, n, p, q in matches:\n",
    "         #   if m.distance < 0.75 * n.distance and m.distance < 0.75 * p.distance and m.distance < 0.75 * q .distance:\n",
    "          #      good_matches.append(m)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        if len(kp1) == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = len(matches) / len(kp1)  # Using the ratio of matches to keypoints as the similarity metric\n",
    "\n",
    "        #similarity = len(good_matches) / len(kp1)  # Using the ratio of good matches to keypoints as the similarity metric (knn)\n",
    "        \n",
    "        # Define a threshold to determine if the asset has been altered\n",
    "        similarity_threshold = 0.55\n",
    "\n",
    "        # Display the similarity and detected alteration status\n",
    "        sim = f\"Similarity: {similarity}\"\n",
    "        cv2.putText(frame, sim, (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "        print(f\"Similarity: {similarity}\")\n",
    "        if similarity < similarity_threshold:\n",
    "            warning = \" Asset altered or damaged!\"\n",
    "            cv2.putText(frame, sim+warning, (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "            print(warning)\n",
    "        good_matches = [] # Emptying the list for the next frame\n",
    "        \n",
    "    # Display the frame with the bounding box\n",
    "    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Asset Alteration Detection\", frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without Tracker\n",
    "\n",
    "# Read the initial frame and select the region of interest (ROI)\n",
    "ret, frame = video_capture.read()\n",
    "bbox = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "# Extract features from the initial state (ROI)\n",
    "roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "kp1, des1 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract features from the current state (ROI)\n",
    "    roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "    kp2, des2 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "    # Perform feature matching\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity = len(matches) / len(kp1)  # Use the ratio of matches to keypoints as the similarity metric\n",
    "\n",
    "    # Define a threshold to determine if the asset has been altered\n",
    "    similarity_threshold = 0.55\n",
    "\n",
    "    # Display the similarity and detected alteration status\n",
    "    print(f\"Similarity: {similarity}\")\n",
    "    if similarity < similarity_threshold:\n",
    "        print(\"Asset altered or damaged!\")\n",
    "\n",
    "    # Display the frame with the bounding box\n",
    "    cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Asset Alteration Detection\", frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without ROI and Tracker\n",
    "\n",
    "# Read the initial frame\n",
    "ret, frame = video_capture.read()\n",
    "\n",
    "# Extract features from the initial frame\n",
    "kp1, des1 = orb.detectAndCompute(frame, None)\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract features from the current frame\n",
    "    kp2, des2 = orb.detectAndCompute(frame, None)\n",
    "\n",
    "    # Perform feature matching\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarity = len(matches) / len(kp1)  # Use the ratio of matches to keypoints as the similarity metric\n",
    "\n",
    "    # Define a threshold to determine if the asset has been altered\n",
    "    similarity_threshold = 0.55\n",
    "\n",
    "    # Display the similarity and detected alteration status\n",
    "    sim = f\"Similarity: {similarity}\"\n",
    "    if similarity < similarity_threshold:\n",
    "            warning = \" Asset altered or damaged!\"\n",
    "            cv2.putText(frame, sim+warning, (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "            print(warning)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Asset Alteration Detection\", frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Modules II and III: Detection of offensive activity and alteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a batch to store the frames\n",
    "frame_batch = []\n",
    "\n",
    "# Read the initial frame and select the region of interest (ROI)\n",
    "ret, frame = video_capture.read()\n",
    "bbox = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "# Initializing the tracker with the defined ROI\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "# Extracting features from the initial state (ROI)\n",
    "roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "kp1, des1 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "# Creating a Brute Force matcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the object tracker\n",
    "    ret, bbox = tracker.update(frame)\n",
    "    if ret:\n",
    "\n",
    "        ### OFFENSIVE ACTTVITY DETECTION: ######################################################\n",
    "        ########################################################################################\n",
    "\n",
    "        frame_batch.append(frame)\n",
    "        # Check if the frame batch size has reached the desired batch size\n",
    "        if len(frame_batch) == data_hyperparams['batch_size']:\n",
    "            # Convert the frame batch to a numpy array\n",
    "            frame_batch = np.array(frame_batch)\n",
    "\n",
    "            # Perform preprocessing on the frame batch\n",
    "            frame_batch = np.array(preprocess(frame_batch))\n",
    "\n",
    "            # Pass the frame batch to the computer vision model for offensive action detection\n",
    "            predictions = model.predict(frame_batch)\n",
    "            Class = BatchClassDecoder(predictions)[1]\n",
    "            print(Class)\n",
    "            cv2.putText(frame, str(Class), (40,40), cv2.FONT_HERSHEY_COMPLEX, 1, (255,0,0),2)\n",
    "            # Clear the frame batch for the next batch of frames\n",
    "            frame_batch = []\n",
    "\n",
    "        ### ALTERATION DETECTION: ##############################################################\n",
    "        ########################################################################################\n",
    "        \n",
    "          # Extract features from the current state (ROI)\n",
    "        roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "        kp2, des2 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "        # Perform feature matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # Perform kNN matching\n",
    "        #matches = bf.knnMatch(des1, des2, k=4)\n",
    "\n",
    "        # Apply ratio test to filter good matches\n",
    "        #good_matches = []\n",
    "        #for m, n, p, q in matches:\n",
    "        #    if m.distance < 0.75 * n.distance and m.distance < 0.75 * p.distance and m.distance < 0.75 * q .distance:\n",
    "        #       good_matches.append(m)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = len(matches) / len(kp1)  # Using the ratio of matches to keypoints as the similarity metric\n",
    "        #similarity = len(good_matches) / len(kp1)  # Using the ratio of good matches to keypoints as the similarity metric (knn)\n",
    "        \n",
    "        # Define a threshold to determine if the asset has been altered\n",
    "        similarity_threshold = 0.55\n",
    "\n",
    "        # Display the similarity and detected alteration status\n",
    "        sim = f\"Similarity: {similarity}\"\n",
    "        cv2.putText(frame, sim, (75,75), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "        print(f\"Similarity: {similarity}\")\n",
    "        if similarity < similarity_threshold:\n",
    "            warning = \" Asset altered or damaged!\"\n",
    "            cv2.putText(frame, sim+warning, (60,60), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "            print(warning)\n",
    "        good_matches = [] # Emptying the list for the next frame\n",
    "        \n",
    "        # Display the frame with the bounding box\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (0, 255, 0), 2)  \n",
    "        \n",
    "\n",
    "\n",
    "        # Displaying the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    # Checking for the 'q' key to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Modules I, II and III: Detection of Human Presence, Offensive Behaviour and Alteration + Scoring and Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the class labels to the corresponding severity scores:\n",
    "def scoreMapper(n):\n",
    "    scores = [6,5,10,8,10,9,5,0,8,8,10,7,7,7]\n",
    "    return scores[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associating each range of scores with a severity level\n",
    "def score2severity(score):\n",
    "    if 9<=score<=10:\n",
    "        return (\"Critical\",5)\n",
    "    elif 8<=score<9:\n",
    "        return (\"Very High\",4)\n",
    "    elif 7<score<=8:\n",
    "        return (\"High\",3)\n",
    "    elif 4<=score<7:\n",
    "        return (\"Medium\",2)\n",
    "    elif 0<score<4:\n",
    "        return (\"Low\",1)\n",
    "    elif score == 0:\n",
    "        return (\"No Incident\",0)\n",
    "    \n",
    "# The idea of the following function is that the severity of the General incident is equal to the highest severity\n",
    "# between the offensive activity incident and the alteration incident. For example, if no offensive activity is\n",
    "# detected (score1 = 0) but the asset is severely altered (score2 = 9), the incident should be considered critical\n",
    "# even if the total score (score1+score2) is just 9/20. This is why we use the max().\n",
    "def Severity(score1, score2):\n",
    "    level = np.max([score2severity(score1)[1], score2severity(score2)[1]])\n",
    "    levels = [\"No Incident\", \"Low\", \"Medium\", \"High\", \"Very High\", \"Critical\"]\n",
    "    return levels[level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_mqtt_notification(incident_type, severity, human_presence, timestamp, image_data):\n",
    "    broker_address = \"maqiatto.com\"  # MQTT broker address\n",
    "    broker_port = 1883  # MQTT broker port\n",
    "    topic_notification = \"womendjia.ivan@gmail.com/anomaly\"  # MQTT topic for notifications\n",
    "    topic_image = \"womendjia.ivan@gmail.com/images\"  # MQTT topic for the image\n",
    "\n",
    "    client = mqtt.Client()  # Create a new MQTT client instance\n",
    "    client.username_pw_set(\"womendjia.ivan@gmail.com\",\"mqttpass\")\n",
    "    \n",
    "    client.connect(broker_address, broker_port)  # Connect to the MQTT broker\n",
    "\n",
    "    # Publish the notification message to the MQTT topic for notifications\n",
    "    notification_message = f\"Incident Type: {incident_type}_Severity Level: {severity}_Human Presence:{human_presence}_Time: {datetime.fromtimestamp(timestamp).strftime('%d-%m-%y %H:%M')}_Image:{image_data}\"\n",
    "    print(\"========sending notification==========\")\n",
    "    print(notification_message)\n",
    "    client.publish(topic_notification, payload=notification_message)  # Publish the MQTT message to the specified topic\n",
    "\n",
    "    # Publish the image data to the MQTT topic for the image\n",
    "    #client.publish(topic_image, payload=image_data)  # Publish the image data as the payload\n",
    "\n",
    "    client.disconnect()  # Disconnect from the MQTT broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a batch to store the frames\n",
    "frame_batch = []\n",
    "\n",
    "# Read the initial frame and select the region of interest (ROI)\n",
    "ret, frame = video_capture.read()\n",
    "bbox = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
    "# Initializing the tracker with the defined ROI\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "# Extracting features from the initial state (ROI)\n",
    "roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "kp1, des1 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "# Creating a Brute Force matcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "# Initializing the timestamp counter\n",
    "i=1\n",
    "score1, score2 = 0,0\n",
    "presence = False\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the object tracker\n",
    "    ret, bbox = tracker.update(frame)\n",
    "    if ret:\n",
    "        ### HUMAN PRESENCE DETECTION: ##########################################################\n",
    "        ########################################################################################\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.array(frame))\n",
    "        with ObjectDetector.create_from_options(options) as detector:\n",
    "            detector.detect_async(mp_image, i)\n",
    "            i+=1\n",
    "\n",
    "            # Defining a binary state variable that indicates whether there is a person near the asset\n",
    "        if DRS.detections[0].categories[0].category_name == 'person':\n",
    "            presence = True\n",
    "        \"\"\"\n",
    "        NOTE: Be very careful with the detector.detect_async() function. Its second argument (frame timestamp)\n",
    "        MUST be GREATER than what the function has processed previously. In other words, If you use incremented \n",
    "        counters as timestamps when you first run the function, the first counter of the next execution MUST be GREATER\n",
    "        than the last counter of the function. The same applies if you use timestamps from the time module.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### ALTERATION DETECTION: ##############################################################\n",
    "        ########################################################################################\n",
    "\n",
    "        # The key idea is to calculate the similarity score for each frame in the batch, and then\n",
    "        # averaging the results to get the mean alteration score for the batch.\n",
    "\n",
    "        batch_similarities = [] # Collection of the similarity scores for the frames in the batch\n",
    "\n",
    "        # Extract features from the current state (ROI)\n",
    "        roi = frame[int(bbox[1]):int(bbox[1] + bbox[3]), int(bbox[0]):int(bbox[0] + bbox[2])]\n",
    "        kp2, des2 = orb.detectAndCompute(roi, None)\n",
    "\n",
    "        # Perform feature matching\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate similarity\n",
    "        if len(kp1) == 0:\n",
    "            similarity = 0\n",
    "        else:\n",
    "            similarity = len(matches) / len(kp1)  # Using the ratio of matches to keypoints as the similarity metric\n",
    "        batch_similarities.append(similarity)\n",
    "\n",
    "        ### OFFENSIVE ACTTVITY DETECTION: ######################################################\n",
    "        ########################################################################################\n",
    "\n",
    "        frame_batch.append(frame)\n",
    "\n",
    "        # Check if the frame batch size has reached the desired batch size\n",
    "        if len(frame_batch) == data_hyperparams['batch_size']:\n",
    "            # Convert the frame batch to a numpy array\n",
    "            frame_batch = np.array(frame_batch)\n",
    "\n",
    "            # Perform preprocessing on the frame batch\n",
    "            frame_batch = np.array(preprocess(frame_batch))\n",
    "\n",
    "            # Pass the frame batch to the computer vision model for offensive action detection\n",
    "            predictions = model.predict(frame_batch)\n",
    "\n",
    "            # Get the class of the prediction\n",
    "            Class = BatchClassDecoder(predictions)[1]\n",
    "            #cv2.putText(frame, str(Class), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (255,0,0),2)\n",
    "\n",
    "            # Getting the probability for a specific class\n",
    "            #probs = BatchProb(predictions)\n",
    "            #print('Fighting score: ',probs[6])\n",
    "            \n",
    "            # computing the severity score for offensive behaviour\n",
    "            score1 = scoreMapper(BatchClassDecoder(predictions)[0])\n",
    "\n",
    "            # Calculating the average similarity score\n",
    "            batch_similarity = np.mean(batch_similarities)\n",
    "\n",
    "            detection_time = time.time() # Time at which the incidents were detected\n",
    "\n",
    "            # Defining a threshold to determine if the asset has been altered\n",
    "            similarity_threshold = 0.4\n",
    "\n",
    "            # Display the similarity and detected alteration status\n",
    "            # cv2.putText(frame, sim, (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255),2)\n",
    "            print(f\"Batch Similarity: {batch_similarity}\")\n",
    "            alteration_severity = \"\"\n",
    "            if 0.7*similarity_threshold <=batch_similarity <=0.85*similarity_threshold:\n",
    "                score2 = 2 # Minor alteration\n",
    "                alteration_severity = \"Minor alteration\"\n",
    "            elif 0.4*similarity_threshold <=batch_similarity <0.7*similarity_threshold:\n",
    "                score2 = 5 # Moderate alteration\n",
    "                alteration_severity = \"Moderate alteration\"\n",
    "            elif 0 <= batch_similarity < 0.4*similarity_threshold:\n",
    "                score2 = 9 # Major alteration\n",
    "                alteration_severity = \"Major alteration\"\n",
    "                \n",
    "            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 30]\n",
    "            # Convert the last frame to JPEG format\n",
    "            _, encoded_image = cv2.imencode(\".jpg\", frame_batch[-1],encode_param)\n",
    "            \n",
    "            # Encode the binary data to base64 string\n",
    "            image_b64 = str(base64.b64encode(encoded_image))\n",
    "            # Clear the frame batch and similarity scores list for the next batch of frames\n",
    "            frame_batch = []\n",
    "            batch_similarities = []\n",
    "  \n",
    "            # Computing the final score\n",
    "            score = score1 + score2\n",
    "\n",
    "            # Determining the level of severity from the score\n",
    "            severity = Severity(score1, score2)\n",
    "            human_presence = \"No\"\n",
    "            if presence:\n",
    "                human_presence = \"Yes\" # Human Presence\n",
    "            # Sending a notification in the MQTT  format depending on the severity: \n",
    "            # Uncomment if you have an MQTT broker set up\n",
    "            if Class != \"Normal\" and alteration_severity != \"\":\n",
    "                send_mqtt_notification(Class + \" and \" + alteration_severity, severity, human_presence, detection_time, image_b64)\n",
    "            if Class == \"Normal\" and alteration_severity != \"\":\n",
    "                pass\n",
    "                send_mqtt_notification(alteration_severity, severity, human_presence, detection_time, image_b64)\n",
    "        # Display the frame with the bounding box\n",
    "        cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (0, 255, 0), 2)\n",
    "        # Displaying the frame\n",
    "        cv2.imshow('Video', frame)\n",
    "    # Checking for the 'q' key to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
